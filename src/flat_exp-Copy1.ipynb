{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "discrete-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from psql_functions import execQuery\n",
    "from miss_data import add_missing_dates, add_missing_counts\n",
    "#from sample_range_query import load_range_queries\n",
    "\n",
    "import re\n",
    "import os\n",
    "owd = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "reflected-classification",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(owd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "empirical-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_range_queries_n_split(file, n_structures):\n",
    "    all_queries = pd.read_csv(file, sep='\\n',header=None).to_numpy().flatten()\n",
    "    split_queries = np.array_split(all_queries, n_structures)\n",
    "    return split_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "confidential-generic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed query and closed connection.\n",
      "Executed query and closed connection.\n"
     ]
    }
   ],
   "source": [
    "param_dic = {\n",
    "    \"host\"      : \"localhost\",\n",
    "    \"database\"  : \"bachelorBesoeg2014\",\n",
    "    \"user\"      : \"postgres\",\n",
    "    \"password\"  : \"password\",\n",
    "    \"port\"      : \"5432\"\n",
    "}\n",
    "\n",
    "query = \"\"\"select time_ from _775147;\"\"\"\n",
    "result = execQuery(param_dic, query)\n",
    "dates = [(date[0]) for date in result]\n",
    "\n",
    "query = \"\"\"select count_ from _775147;\"\"\"\n",
    "result = execQuery(param_dic, query)\n",
    "\n",
    "counts = [(count[0]) for count in result]\n",
    "\n",
    "all_dates = add_missing_dates(dates)\n",
    "all_counts =  add_missing_counts(counts, dates, all_dates)\n",
    "\n",
    "n_data_structures = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "continuing-valentine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nquery_dates = ('2014-01-02','2014-01-9')\\nepsilon = 0.7\\nFLAT_OLH = OLH_flat(epsilon, all_dates[:32], all_counts[:32])\\nprint(FLAT_OLH.answer(query_dates))\\nprint(FLAT_OLH.real_answer(query_dates))\\nprint(FLAT_OLH.noise_counts)\\nprint(FLAT_OLH.var)\\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import laplace\n",
    "from datetime import datetime\n",
    "from datetime import timedelta \n",
    "\n",
    "class OLH_flat:\n",
    "    def __init__(self, epsilon, dates, counts):\n",
    "        \"\"\"Setup of the datastructere\n",
    "        Parameters:\n",
    "        T (int): The lenght of the stream\n",
    "        epsilon (float): The height of the full binary tree. \n",
    "        dates (Array): The dates of the stream\n",
    "        counts (Array): The count for each of the dates\n",
    "        Returns:\n",
    "        A epsilon differintial datastructe\n",
    "        \"\"\"\n",
    "        \n",
    "        self.epsilon = epsilon\n",
    "        self.all_dates = dates\n",
    "        self.all_counts = counts\n",
    "        \n",
    "        if len(dates) < (dates[-1]-dates[0]).days:\n",
    "            print('here')\n",
    "            self.all_dates = self.__add_missing_dates(dates)\n",
    "            self.all_counts = self.__add_missing_counts(counts,dates)\n",
    "            \n",
    "        #Make dict for date indexing\n",
    "        values = np.arange(0,len(self.all_dates))\n",
    "        zip_iterator = zip(self.all_dates, values)\n",
    "        self.idx_dict =  dict(zip_iterator)\n",
    "        \n",
    "        self.noise_counts = self.__process(self.all_dates, self.all_counts)\n",
    "        #Check if we are we have missing dates.\n",
    "        \n",
    "        self.p = np.exp(self.epsilon)/(np.exp(self.epsilon)+len(self.all_dates)-1)\n",
    "        self.var = self.OLH_var(self.p, len(self.all_dates))\n",
    "        \n",
    "    def __add_missing_dates(self, old_dates):\n",
    "        \"\"\"Add missing dates in a list\n",
    "        Parameters:\n",
    "        old_dates (list of datetime.date): List of dates that is not countious\n",
    "        Returns:\n",
    "        List of countious starting with the first value of \n",
    "        \"\"\"\n",
    "        start_date = old_dates[0]\n",
    "        end_date = old_dates[-1]\n",
    "        all_dates = pd.date_range(start = start_date, end = end_date).to_pydatetime().tolist()\n",
    "        return [(date.date()) for date in all_dates]\n",
    "    \n",
    "    def OLH_var(self, p, N):\n",
    "        return 4*p*(1-p)/(N*(2*p-1)**2)\n",
    "    \n",
    "    def __add_missing_counts(self, old_counts, old_dates):\n",
    "        \"\"\"Adds 0 to the list of counts where there was missing dates\n",
    "        Parameters:\n",
    "        old_counts (list of int): List counts for each day with \n",
    "        old_dates (list of datetime.date): List of dates that is not countious\n",
    "        Returns:\n",
    "        List of countious starting with the first value of \n",
    "        \"\"\"\n",
    "        zip_iterator = zip(old_dates, old_counts)\n",
    "        missing_dict =  dict(zip_iterator)\n",
    "        all_counts = np.zeros(len(self.all_dates))\n",
    "        for i, date in enumerate(self.all_dates):\n",
    "            val = missing_dict.get(date, 0)\n",
    "            all_counts[i] = val\n",
    "            \n",
    "        return all_counts\n",
    "    \n",
    "    def OLH_func(self, x, g):\n",
    "        if np.random.uniform(0,1) < np.exp(self.epsilon)/(np.exp(self.epsilon)+g-1):\n",
    "            #print('if')\n",
    "            return x\n",
    "        else:\n",
    "            return np.random.randint(low = 0, high = g)\n",
    "    \n",
    "    def OLH_aggre(self, count, N, g):\n",
    "        p = np.exp(self.epsilon)/(np.exp(self.epsilon)+g-1)\n",
    "        #print(p - 1/g)\n",
    "        #print(f'p = {p}')\n",
    "        return (count - (1-p)*N/g) / (p)\n",
    "\n",
    "    def OLH_answer(self, count, N, g):\n",
    "        p = np.exp(self.epsilon)/(np.exp(self.epsilon)+g-1)\n",
    "        #print(p - 1/g)\n",
    "        #print(f'p = {p}')\n",
    "        return (count- N/g) / (p)\n",
    "\n",
    "    def __process(self, dates, counts):\n",
    "        OLH_count = np.zeros(len(counts))\n",
    "        D = len(dates)\n",
    "        \n",
    "        for idx, count in enumerate(counts):\n",
    "            for i in range(0,int(count)):\n",
    "                response = self.OLH_func(idx, D)\n",
    "                OLH_count[response] = OLH_count[response] + 1\n",
    "                \n",
    "        return OLH_count\n",
    "    \n",
    "    def answer(self, dates):\n",
    "        \"\"\"Calculates the path of index in full binary string\n",
    "\n",
    "        Parameters:\n",
    "        dates (tuple of string): Two dates in the format string 2000-12-19. \n",
    "\n",
    "        Returns:\n",
    "        float: The private range count\n",
    "        \"\"\"\n",
    "        N = np.sum(self.noise_counts)\n",
    "        D = len(self.noise_counts)\n",
    "        if (len(dates) < 2):\n",
    "            #There is only one date\n",
    "            date_obj_0 = datetime.strptime(dates[0],'%Y-%m-%d').date()\n",
    "\n",
    "            idx = self.idx_dict[date_obj_0]\n",
    "            noise_count = self.noise_counts[idx]\n",
    "            return self.OLH_answer(noise_count, N, D)\n",
    "            \n",
    "        else:\n",
    "            date_obj_0 = datetime.strptime(dates[0],'%Y-%m-%d').date()\n",
    "            date_obj_1 = datetime.strptime(dates[1],'%Y-%m-%d').date()\n",
    "            idx_0 = self.idx_dict[date_obj_0]\n",
    "            idx_1 = self.idx_dict[date_obj_1]\n",
    "            #print(idx_0)\n",
    "            #print(idx_1)\n",
    "            #idx_0 is not 0\n",
    "            noise_sum = 0.0\n",
    "            for i in range(idx_0, idx_1+1):\n",
    "                #print(i)\n",
    "                #print(self.OLH_answer(self.noise_counts[i], N, D))\n",
    "                noise_sum = noise_sum + self.OLH_aggre(self.noise_counts[i], N, D)\n",
    "            return noise_sum\n",
    "    \n",
    "    def real_answer(self, dates):\n",
    "        if len(dates) < 2:\n",
    "            date_obj_0 = datetime.strptime(dates[0],'%Y-%m-%d').date()\n",
    "            return self.all_counts[self.idx_dict[date_obj_0]]\n",
    "        else:\n",
    "            date_obj_0 = datetime.strptime(dates[0],'%Y-%m-%d').date()\n",
    "            date_obj_1 = datetime.strptime(dates[1],'%Y-%m-%d').date()\n",
    "            #print(self.all_counts[self.idx_dict[date_obj_0]: self.idx_dict[date_obj_1]+1])\n",
    "            sum_ = np.sum(self.all_counts[self.idx_dict[date_obj_0]: self.idx_dict[date_obj_1]+1])  \n",
    "            return sum_ \n",
    "\"\"\"\n",
    "query_dates = ('2014-01-02','2014-01-9')\n",
    "epsilon = 0.7\n",
    "FLAT_OLH = OLH_flat(epsilon, all_dates[:32], all_counts[:32])\n",
    "print(FLAT_OLH.answer(query_dates))\n",
    "print(FLAT_OLH.real_answer(query_dates))\n",
    "print(FLAT_OLH.noise_counts)\n",
    "print(FLAT_OLH.var)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "integral-hands",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir(os.getcwd()+'/'+'range_queries/flat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "prerequisite-preliminary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flat_N=1024_r=44.csv',\n",
       " 'flat_N=128_r=9.csv',\n",
       " 'flat_N=2048_r=64.csv',\n",
       " 'flat_N=256_r=24.csv',\n",
       " 'flat_N=32_r=6.csv',\n",
       " 'flat_N=512_r=35.csv',\n",
       " 'range_queries_flat_N=1024_r=44.csv',\n",
       " 'range_queries_flat_N=128_r=9.csv',\n",
       " 'range_queries_flat_N=2048_r=64.csv',\n",
       " 'range_queries_flat_N=256_r=24.csv',\n",
       " 'range_queries_flat_N=32_r=6.csv',\n",
       " 'range_queries_flat_N=512_r=35.csv']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = [f for f in os.listdir('.') if re.match(r'.*\\.csv', f)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "academic-chaos",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_32_flat = []\n",
    "n_128_flat = []\n",
    "n_256_flat = []\n",
    "n_512_flat = []\n",
    "n_1024_flat = []\n",
    "n_2048_flat = []\n",
    "\n",
    "for f in files:\n",
    "    if 'N=32' in f:\n",
    "        n_32_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=128' in f:\n",
    "        n_128_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=256' in f:\n",
    "        n_256_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=512' in f:\n",
    "        n_512_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=1024' in f:\n",
    "        n_1024_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=2048' in f:\n",
    "        n_2048_flat.append(load_range_queries_n_split(f, n_data_structures))\n",
    "\n",
    "n_32_flat = [item for sublist in n_32_flat for item in sublist]\n",
    "n_128_flat = [item for sublist in n_128_flat for item in sublist]\n",
    "n_256_flat = [item for sublist in n_256_flat for item in sublist]\n",
    "n_512_flat = [item for sublist in n_512_flat for item in sublist]\n",
    "n_1024_flat = [item for sublist in n_1024_flat for item in sublist]\n",
    "n_2048_flat = [item for sublist in n_2048_flat for item in sublist]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "structured-audit",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hh_N=1024_B=2_r=402.csv',\n",
       " 'hh_N=1024_B=3_r=296.csv',\n",
       " 'hh_N=1024_B=4_r=202.csv',\n",
       " 'hh_N=2048_B=2_r=485.csv',\n",
       " 'hh_N=2048_B=3_r=294.csv',\n",
       " 'hh_N=2048_B=4_r=288.csv',\n",
       " 'hh_N=256_B=2_r=44.csv',\n",
       " 'hh_N=256_B=3_r=217.csv',\n",
       " 'hh_N=256_B=4_r=130.csv',\n",
       " 'hh_N=512_B=2_r=325.csv',\n",
       " 'hh_N=512_B=3_r=217.csv',\n",
       " 'hh_N=512_B=4_r=200.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(owd)\n",
    "os.getcwd()\n",
    "os.chdir(os.getcwd()+'/'+'range_queries/local_hh')\n",
    "os.getcwd()\n",
    "files = [f for f in os.listdir('.') if re.match(r'.*\\.csv', f)]\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-narrow",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "accredited-sussex",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_256_hh = []\n",
    "n_512_hh = []\n",
    "n_1024_hh = []\n",
    "n_2048_hh = []\n",
    "\n",
    "for f in files:\n",
    "    if 'N=256' in f:\n",
    "        n_256_hh.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=512' in f:\n",
    "        n_512_hh.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=1024' in f:\n",
    "        n_1024_hh.append(load_range_queries_n_split(f, n_data_structures))\n",
    "    if 'N=2048' in f:\n",
    "        n_2048_hh.append(load_range_queries_n_split(f, n_data_structures))\n",
    "\n",
    "n_256_hh = [item for sublist in n_256_hh for item in sublist]\n",
    "n_512_hh = [item for sublist in n_512_hh for item in sublist]\n",
    "n_1024_hh = [item for sublist in n_1024_hh for item in sublist]\n",
    "n_2048_hh = [item for sublist in n_2048_hh for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "touched-macintosh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(n_256_hh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cardiac-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_answers(model, query):\n",
    "    estimation = model.answer(query)\n",
    "    correct = model.real_answer(query)  \n",
    "    return estimation, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "baking-office",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nn_32_flat = []\\nn_128_flat = []\\nn_256_flat = []\\nn_512_flat = []\\nn_1024_flat = []\\nn_2048_flat = []\\n\\nn_256_hh = []\\nn_512_hh = []\\nn_1024_hh = []\\nn_2048_hh = []\\n\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epsilons = np.array([2,1.4,1.2,1,0.8,0.6,0.5,0.3])\n",
    "n = np.array([32,128,256,512,1024,2048])\n",
    "os.chdir(owd)\n",
    "#We have 50 datastruces\n",
    "\n",
    "for e in epsilons:\n",
    "    for N in n:\n",
    "        csv_name_est_flat = 'results/sample_querys/flat/' + f'est_flat_queries_e={e}_N={N}.csv'\n",
    "        csv_name_cor_flat = 'results/sample_querys/flat/' + f'cor_flat_queries_e={e}_N={N}.csv'\n",
    "        \n",
    "        csv_name_est_hh = 'results/sample_querys/flat/' + f'est_hh_queries_e={e}_N={N}.csv'\n",
    "        csv_name_cor_hh = 'results/sample_querys/flat/' + f'cor_hh_queries_e={e}_N={N}.csv'\n",
    "        \n",
    "        answers_flat = []\n",
    "        correct_answers_flat = []\n",
    "        \n",
    "        answers_hh = []\n",
    "        correct_answers_hh = []\n",
    "        \n",
    "        for i in range(0,n_data_structures):\n",
    "            FLAT_OLH = OLH_flat(e, all_dates[:N], all_counts[:N])\n",
    "            \n",
    "            if N == 32:\n",
    "                for query in n_32_flat[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)\n",
    "\n",
    "            if N == 128:\n",
    "                for query in n_128_flat[i]:\n",
    "\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)\n",
    "\n",
    "                \n",
    "            if N == 256:\n",
    "                for query in n_32_flat[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)             \n",
    "\n",
    "            if N == 512:\n",
    "                for query in n_512_flat[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)\n",
    "                \n",
    "                for query in n_512_hh[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "                    \n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_hh.append(est)\n",
    "                    correct_answers_hh.append(cor)\n",
    "\n",
    "            if N == 1024:\n",
    "                for query in n_1024_flat[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)\n",
    "                \n",
    "                for query in n_1024_hh[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_hh.append(est)\n",
    "                    correct_answers_hh.append(cor)\n",
    "\n",
    "            if N == 2048:\n",
    "                for query in n_2048_flat[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_flat.append(est)\n",
    "                    correct_answers_flat.append(cor)\n",
    "                \n",
    "                for query in n_2048_hh[i]:\n",
    "                    dates_split = query.split(', ')\n",
    "                    res = tuple(dates_split)\n",
    "\n",
    "                    est, cor = model_answers(FLAT_OLH, res)\n",
    "                    answers_hh.append(est)\n",
    "                    correct_answers_hh.append(cor)\n",
    "            \n",
    "            \n",
    "        np.savetxt(csv_name_est_flat, answers_flat, delimiter=',')\n",
    "        np.savetxt(csv_name_cor_flat, correct_answers_flat, delimiter=',')\n",
    "        \n",
    "        np.savetxt(csv_name_est_hh, answers_hh, delimiter=',')\n",
    "        np.savetxt(csv_name_cor_hh, correct_answers_hh, delimiter=',')\n",
    "\n",
    "\"\"\"\n",
    "n_32_flat = []\n",
    "n_128_flat = []\n",
    "n_256_flat = []\n",
    "n_512_flat = []\n",
    "n_1024_flat = []\n",
    "n_2048_flat = []\n",
    "\n",
    "n_256_hh = []\n",
    "n_512_hh = []\n",
    "n_1024_hh = []\n",
    "n_2048_hh = []\n",
    "\n",
    "\"\"\"\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "played-capability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exact-california",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "lovely-proof",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFLAT_OLH = OLH_flat(epsilon, all_dates[:32], all_counts[:32])\\nprint(FLAT_OLH.answer(query_dates))\\nprint(FLAT_OLH.real_answer(query_dates))\\nprint(FLAT_OLH.noise_counts)\\nprint(FLAT_OLH.var)\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "FLAT_OLH = OLH_flat(epsilon, all_dates[:32], all_counts[:32])\n",
    "print(FLAT_OLH.answer(query_dates))\n",
    "print(FLAT_OLH.real_answer(query_dates))\n",
    "print(FLAT_OLH.noise_counts)\n",
    "print(FLAT_OLH.var)\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
